## 一、原理
做文本相关度打分的公式，主要思想就是计算一个query里面所有词和文档的相关度，每个词的相关度以idf为权重，把加权后的分数累加作为最后分数。公式如下：
<center> 
<img src=figure/1.png width = 55% div align=center> 
</center> 
R(qi,d)是每个词和文档的相关度值，其中qi代表每个词，d代表相关的文档，Wi是这个词的权重，然后所有词的乘积再做累加。  

1、Wi  
Wi默认是idf值，公式如下(和tf-idf中的idf一样)。N是文档总数，n(qi)是包含该词的文档数，0.5是调教系数，避免n(qi)为0的情况，从这个公式可以看出N越大，n(qi)越小的话idf值越大，这也符合了"词的重要程度和其出现在总文档集合里的频率成反比"的思想，取个log是为了让idf的值受N和n(qi)的影响更加平滑。
<center> 
<img src=figure/2.png width = 48% div align=center> 
</center> 

2、R(qi,d)   
公式如下。k1，k2，b都是调节因子，一般k1=2，k2=1，b=0.75, fi是词在文档中的次数，qfi代表词在查询语句里的次数，dl是文档长度，avgdl是文档平均长度。乘积的左边因数代表词在文档中的次数关系，乘积的右边因数代表词在查询语句中的次数关系：   
<center> 
<img src=figure/3.png width = 70% div align=center> 
</center> 

K: 随着query长度的增加而增加，因为K在分母上，说明句子长度越长，他每个词匹配到的document的相关度就越低；   

<img src=figure/4.png width = 15% div align=center>： 因为K在2上下浮动，所以这个式子的值随着fi的增加而增加，但增加的幅度越来越小，逐渐逼近(k1+1)。意思就是qi在document中出现的次数越多，相关性就越大，意义与tf一致,但做了平滑处理，而不是线性增长；   

<img src=figure/5.png width = 15% div align=center>：这个式子的值随着qfi的增加而增加，但增加的幅度越来越小，逐渐逼近(k2+1)。意思就是qi在query中出现的次数越多，相关性就越大，意义与tf一致,但做了平滑处理，而不是线性增长。  

3、影响BM25公式的因数有：
a、idf，idf越高分数越高  
b、tf tf越高分数越高  
c、dl/avgdl 如果该文档长度在文档水平中越高则分数越低   

## 二、一些思考
1、BM25与tf-idf思想类似，都是用idf当成词的权重，tf衡量相关性。这些方法可以通过匹配同样的词来衡量文本的相关性，但是捕捉不到语义相近但形式不同的词(如爱好和兴趣、朋友和伙伴)。  
2、实现这个算法的顺序：数据清洗、query和document切词、**建立分词到document的倒排索引**、计算相关度、排序。建立倒排可以减少很多计算时间。

## 三、参考
博客：https://www.cnblogs.com/hdflzh/p/4034602.html