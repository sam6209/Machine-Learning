# GBDT
#### 一、基本背景  
1、最终的总分类器是将每轮训练得到的弱分类器加权求和得到的(也就是加法模型)。
<center> 
<img src=figure/1.png width = 30% div align=center> 
</center> 

2、关键：利用损失函数的负梯度在当前模型的值，作为回归问题提升树算法中的残差的近似值，拟合一颗回归树；  
3、无论是回归还是分类，使用的都是CART回归树(类似于XGBoost)。因为需要在每轮迭代中拟合损失函数在当前模型下的负梯度，使用CART分类树的话在这没有意义；

#### 二、原理
1、提升树(boosting tree) 
<center> 
<img src=figure/2.png width = 90% div align=center> 
</center> 
注：由于回归问题的提升树损失函数采用的是平方损失，所以在每轮迭代中只需拟合当前模型的残差。

2、回归问题  
<center> 
<img src=figure/3.png width = 950% div align=center> 
</center> 
注：  
a、初始化f0时,最小化损失函数，而不是取0(和XBoost不同，XGBoost f0直接取0)；  
b、在每轮迭代拟合回归树的时候，用的是负梯度；在决定叶子节点上的值的时候，考虑的则是使损失函数最小化(boost方法因为拟合回归树的时候，用的是残差，因为损失函数是平方损失，拟合得到的回归树，其叶子节点上的值就是使得损失函数最小化)。

3、二分类问题  
GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。  
为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。
将 L(y,f(x))=log(1+exp(−yf(x)))代入回归问题中的公式即可。

4、多分类问题(暂无)

#### 三、思考
1.Gradient Boosting：每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少(当然也可以变向理解成每个新建的模型都给予上一模型的误差给了更多的关注)，与传统Boost对正确、错误的样本进行直接加权还是有区别的；  
2、非线性(相对于逻辑回归、线性回归)；  
3、既可以分类，又可以回归；  
4、由于弱分类器之间存在依赖关系，模型不能并行训练；  

#### 四、参考
李航：《统计学习原理》  
博客：https://www.cnblogs.com/pinard/p/6140514.html  
博客：https://www.cnblogs.com/ModifyRong/p/7744987.html