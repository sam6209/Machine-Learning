# 强化学习  
#### 写在前面：
监督学习学习数据分布的内在规律，但需要标注好的数据，且有明确的类别或者数值；  
但是在很多需要作出渐进决策及控制的问题中，我们很难提供这种明确的监督，去告诉算法什么是正确、什么是错误。比如我们建造了一个四腿机器人，然后尝试通过编程让它行走。从一开始我们就无法定义对于行走来说，什么是正确的动作。所以也就无法为算法提供一个明确的监督方案令其模仿了。  
在强化学习中，我们只会给算法提供一个奖励函数，这个函数会告诉算法在什么情况下是做的好，在什么情况下是做的不好，比如在四腿机器人的例子中，当机器人向前行走时奖励函数会给算法正面的反馈，而当机器人无故后退或翻倒时函数则会给算法负面的反馈。而学习算法的任务就是自主发现“通过做出怎样的动作才能获得更多的奖励”。  

## 一、马尔可夫决策过程
一个马尔可夫决策过程就是一个元组(S,A,{Psa,γ,R})，其中：  
S是一个状态(states)的集合(在直升机自动驾驶的例子中，就代表了直升机所有可能的位置和姿态);   
A是一个动作(action)的集合(在直升机自动驾驶的例子中，就代表了通过操作杆可以对直升机做出的所有动作);  
Psa是状态转换概率，对每个状态s∈S和动作a∈A，Psa是一个在状态空间上的分布。简单的说，就是如果在状态s下发生了动作a，“那么下一步所将变为哪一个状态”的概率分布就由Psa确定;  
γ∈[0,1)称为折扣因子(discount factor)，用来调整未来奖励与当作奖励之间的权重;   
R:S×A→ℝ就是奖励函数(有时只把奖励函数当做关于状态S的函数R:S→ℝ)
    
MDP过程：从状态s0开始，选择一个动作a0∈A。动作的结果就是产生一个随机的状态转换，转换到下一个状态s1，而s1～Ps0a0(也就是事件“转换到下一个状态”服从由当前状态s0,a0确定的分布)。接下来做出下一个动作a1，a1的发生使得MDP转换到下一个状态s2～Ps1a0。总过程如下：  
<center> 
<img src=figure/1.png width = 80% div align=center> 
</center> 

MDP在做出动作a0,a1,⋯后从状态s0,s1中得到的“总收益”为：
<center>R(s0,a0)+γR(s1,a1)+γ2R(s2,a2)+⋯ </center> 

也可以写成只与状态相关的函数：
<center> R(s0)+γR(s1)+γ2R(s2)+⋯ </center> 
大多数情况都使用较为简单的状态奖励函数R(s)。  
在强化学习中，我们的目标是按照时间顺序依次选择动作，使得整个MDP获得的“总收益”最大：
<center> E[R(s0)+γR(s1)+γ2R(s2)+⋯]  </center> 
注意到第t步的奖励被折扣因子γt降低了，于是，为了保证期望值足够大，我们倾向于尽可能早的获得正反馈(并让负反馈到来的越晚越好)。在经济学应用领域，R(⋅)代表赚到的钱，γ也有自然的解释——利率(也就是今天的美元比明天的美元更值钱)。   

**策略(policy)**是一个函数π:S→A，表示从状态到动作的映射。不论何时，只要处于状态s，就采用动作a=π(s)，我们称这个过程为执行策略π(在复杂模型中，策略不止通过当前状态，还可能通过过去的状态及动作，对下一步动作做出判断；而在本课程中，我们仅使用当前状态作为策略的输入)。为策略π定义价值函数（value function）：  
<center> 
<img src=figure/2.png width = 70% div align=center> 
</center> 

Vπ(s)是从状态s开始依照策略π所得到的折扣奖励的预期总收益。（式中的这种标记法理论上说是不正确的，因为π并不是随机变量，不过这种写法在文献中是标准记法。）这个式子由两部分组成：  
前半部分为**即时奖励(immediate reward)** R(s)，即从状态s开始时直接得到的奖励；  
称后半部分为**未来奖励(future reward)**，即当前状态之后的所有步骤决策所产生的折扣奖励的总收益。  
这个式子可以写成(为了方便，我们暂时定义映射s0→s, s1→s′):
<center> 
<img src=figure/3.png width = 60% div align=center> 
</center> 
这是一个递归定义，可以按照递归式写作
<center> 
<img src=figure/4.png width = 50% div align=center> （1）
</center> 
我们之所以不把后半部分直接写成γVπ(s′)，是因为在位于前一个状态时，下一个状态是一个随机变量。所以上式的未来奖励部分写成了期望的定义式：
<center> 
<img src=figure/5.png width = 19% div align=center> 
</center> 
也就是MDP在执行第一步之后，在未来能够获得的折扣奖励的预期总收益。  
(1)式称作贝尔曼方程(Bellman Equation)，这是我们在解决MDP问题时用到的主要方程之一。我们可以说，对于给定的策略π，其价值函数Vπ满足贝尔曼方程。贝尔曼方程可以高效的解出Vπ，尤其是对有限状态的MDP(|S|<∞)，我们可以为每一个状态s计算一次Vπ(s)。如此就可以得到一个由“关于Vπ(s)的有|S|个变量(每个状态s∈S都需要一个预期总收益Vπ(s))同时具有|S|个方程(每个状态s∈S的价值函数Vπ(s)都有一个方程)的线性方程”组成线性方程组，进而通过这些方程解出每一个Vπ(s)。  
我们再给出**最优价值函数(optimal value function)**的定义：
<center> 
<img src=figure/6.png width = 28% div align=center> 
</center> 
这是在状态s时，对于所有可能的策略，能够使折扣奖励的总预期收益最大化的策略π下，得到的总收益。当然，这个最优价值函数也有一个对应的贝尔曼方程：
<center> 
<img src=figure/7.png width = 50% div align=center> （2）
</center> 
这也引出了最佳策略的定义π∗:S→A为
<center> 
<img src=figure/8.png width = 45% div align=center>  (3)
</center> 
也就是说π∗(s)给了我们一个使(2)式中使max部分取到最大值时a的取值。  
那么，对于所有状态s以及所有策略π，有：
<center> 
<img src=figure/9.png width = 35% div align=center> 
</center> 
前面的等式说明Vπ∗(即策略π∗的价值函数)等于“对于所有状态s取到最优的价值函数V∗”。后面的不等式说明π∗的值比任何策略的值都要大。也就是说(3)式定义的π∗是最优策略。

注意到π∗的一个有趣的属性——它是对于所有状态s的最优策略。这并不是说，如果从状态s开始，就有一个针对s的最优策略；如果从状态s′开始，就会采取别的针对s′的最优策略。而是说令(1)式取到最大值的的π∗是对所有状态s而言的，这意味着不论MDP从什么状态开始，我们都可以用策略π∗

## 二、值迭代与策略迭代
假设MDP具有有限的状态以及有限的动作空间(|S|≤ ∞,|A|≤ ∞)。  
1、值迭代(value iteration)：  
a、对于每个状态s，以V(s):=0初始化(于在内存中创建了大小为|S|的状态向量，用0⃗初始化)。  
b、重复直到收敛：{  
对于每个状态,更新
<center> <img src=figure/10.png width = 40% > </center>}   
注意，本小节介绍的算法都是在状态转换概率Psa及奖励函数R已知时使用的。  
这个算法可以看做是不停的尝试使用贝尔曼方程更新价值函数。对于内部的循环，有两种更新方法：  
可以先计算每个s的V(s)，然后用这些新算出来的值代替所有的旧值(用新计算的状态向量更新第1步中的状态向量)，这也称作同步（synchronous）更新。用对当前价值函数的估值映射到新的估值上；  
可以遍历状态s（按某种固定顺序），每次更新一个值（每次V(s)的计算都从状态向量中取最新的分量，得出结果后直接更新状态向量中想要的分量），这也称作异步（asynchronous）更新。

2、策略迭代   
在MDP中还有一种用于计算最优策略的标准算法，称为策略迭代（policy iteration）：
随机初始化策略π；
重复直到收敛：{   
(a) 令V:=Vπ(通过贝尔曼方程组求解);  
(b) 对每一个状态s，令<center><img src=figure/11.png width = 35% div align=center></center> 
}   
内部的循环会不停的计算当前策略下的价值函数，然后使用当前的价值函数更新策略。而步骤(a)可以通过求解贝尔曼方程组得到——也就是在策略π给定时，有|S|个变量Vπ(s)，以及|S|个方程的线性方程组。  
在有限的迭代之后，V会收敛于V∗且π会收敛于π∗。

## 三、模型参数的估计
到目前为止，我们讨论的MDP以及MDP算法都是建立在状态转换概率以及奖励函数已知的前提下。但是，在现实问题中，状态转换概率和奖励函数可能不是明确的已知条件，那么我们必须从数据中估计这些量。
在实际操作中，除非MDP过程终止（比如在倒置钟摆问题中，在钟摆倒下时MDP终止），否则每一个试验将一直运行下去；或者可能运行很多但是有限步后停下。有了这些MDP试验的“经验”，我们就可以推导出状态转换概率的最大似然估计：
<center> <img src=figure/12.png width = 50% > </center>
如果在应用上式时得到的比值为0/0时（比如在状态s下从未执行过动作a时），我们可以简单的将Psa(s′)估计为1|S|(即将Psa估计为在所有状态上的均匀分布)。   
使用类似的过程也可以处理奖励函数R未知的情况，我们可以使用所有状态s下的平均奖励来估计状态s预期即时奖励R(s)。  
在得到MDP的模型之后，我们可以带入估计出的状态转换概率和奖励函数，使用值迭代或策略迭代解出MDP的最佳策略。举个例子，联合使用模型估计和值迭代，在状态转换概率未知的情况下学习MDP，可以使用下面的算法：

随机初始化策略π；  
重复 {  
(a) 在MDP中按照策略π执行一些试验；  
(b) 使用在MDP的试验中积累的“经验”，更新对Psa的估计（如果可以的话也更新R）；  
(c) 使用估计的状态转换概率和奖励函数，应用值迭代，估计新的价值函数V；  
(d) 使用关于V的贪心策略更新π；  
}  

注意到对于上面这个特定算法，有一个能够大幅优化性能的方法：在内部循环使用值迭代的步骤里，我们不像值迭代定义的那样用V=0做初始化，而使用算法上一步迭代得到的解做初始化，这样就给值迭代过程了一个更好的起点，能够使值迭代过程收敛更加迅速。

#### 参考资料
斯坦福机器学习课：https://open.163.com/movie/2008/1/2/N/M6SGF6VB4_M6SGKSC2N.html   
博客：http://nbviewer.jupyter.org/github/zlotus/notes-LSJU-machine-learning/blob/master/chapter16.ipynb 
   
   