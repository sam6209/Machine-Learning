# XGBoost使用
## 写在前面
这篇算上一篇论文总结的姊妹篇吧，这篇讲XGBoost实用。  
XGBoost论文总结:https://github.com/sam6209/Machine-Learning/blob/master/machine_learning/XGBoost/XGBoost.md

## 一、特征工程
1、连续特征：不需要处理，直接输入。连续特征一定不要分桶，因为回归树会自动去寻找最好的切分点(在使损失函数减少最多的那个点分裂)；  
2、无序的离散特征：如省份，one-hot处理；  
3、有序的离散特征：如年龄，当成连续特征直接输入；  
4、libsvm：数据必须转成lbsvm格式才能训练，label index:value 格式如下    
1 1:0.167 2:1 3:-0.333 … 10:0.922   
5、缺失值：libsvm可兼容缺失值, 只需缺少相应的index:value。

## 二、重要参数
1、一般参数：  
booster [default=gbtree]:可选项为gbtree，gblinear或dart；  
其中gbtree和dart是使用基于回归树模型的，而gblinear是使用基于线性模型的；(dart在回归树的基础上借鉴了神经网络的dropout方法做了优化；论文中说基分类器用回归树，实现里面加了线形模型？？)  

silent [default=0]：0表示输出运行信息，1表示不输出；  

nthread [如果不进行设置，默认是最大线程数量]：表示XGBoost运行时的并行线程数量；(对应论文中特征粒度的并行)  

2、模型参数：  
eta [default=3]:对应于论文中的Shrinkage，和梯度下降里的learning rate对应。  

max_depth [default=6]:树的深度。因为是boosting方法，使用了多棵树去拟合，所以树的深度可以不需要太深，大了容易过拟合；  

subsample [default=1]：控制每棵树对于样本随机抽样的比例，典型值：0.5~1；  

colsample_bytree [default=1]: 列抽样，控制每棵树随机抽样的列数的占比，典型值：0.5～1；  

lambda [default=1]：权重的L2正则化；  
alpha [default=1]：权重的L1的正则化；  

3、学习任务参数：  
objective [default=reg：linear] :这个参数定义需要被最小化的损失函数。  
常用的值有：  
reg:linear: –线性回归。(mse?)   
reg:logistic 二分类的逻辑回归，返回预测的概率非类别。(logloss?)   
binary:logistic： 二分类的逻辑回归问题，输出为概率。 (logloss?，reg:logistic和binary:logitstic都返回预测的概率，两者相等)  
multi:softmax使用softmax的多分类器，返回预测的类别。在这种情况下，你还要多设置一个参数：num_class类别数目。(softmax的损失函数?) 

eval_metric [default according to objective] ：默认根据objective参数  
rmse：root mean square error，回归问题使用   
logloss：分类用  
auc:不平衡数据使用

4、注：  
num_round：boosting迭代计算次数。每迭代一遍样本生成一棵树，所以这个参数就是树的数目。网上不少博客都是随意提了一下这个参数，我在调参数的时候发现这个参数对AUC影响挺大，在欠拟合的情况下，每增加一棵树就可以减少一部分偏差。   
 
## 三、简单代码
```
import xgboost as xgb

# load file from text file, also binary buffer generated by xgboost
dtrain = xgb.DMatrix('../data/agaricus.txt.train')
dtest = xgb.DMatrix('../data/agaricus.txt.test')

# specify parameters via map, definition are same as c++ version
param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic'}

# specify validations set to watch performance
watchlist = [(dtest, 'eval'), (dtrain, 'train')]
num_round = 2
bst = xgb.train(param, dtrain, num_round, watchlist)

# this is prediction
preds = bst.predict(dtest)
```

## 四、调参 
1、优先调比较重要的参数，如eta，其他不那么重要的参数可先取默认值；  
2、比较重要的参数先大范围粗粒度搜索，确定到一个相对小的区间，再小范围精确搜索，确定到某个组合。  
3、调不那么重要的参数，如 num_round、colsample_bytree、max_depth、lambda等。

## 五、参考
官方文档：https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters  

官方代码：https://github.com/dmlc/xgboost/blob/master/demo/guide-python/basic_walkthrough.py   

博客:https://blog.csdn.net/m_buddy/article/details/79337492   




